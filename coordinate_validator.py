#!/usr/bin/env python3
"""
ë°”ìš´ë”© ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ê²€ì¦ ë„êµ¬
- JSON ë°ì´í„°ì˜ ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ê²€ì¦
- ì‹œê°í™”ë¥¼ í†µí•œ ì •ì„±ì  ê²€ì¦
- ì¢Œí‘œ ë³€í™˜ ë¡œì§ ê²€ì¦
"""

import json
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from pathlib import Path
import argparse

class BBoxValidator:
    def __init__(self, json_file_path, video_file_path=None):
        """
        Args:
            json_file_path: ì–´ë…¸í…Œì´ì…˜ JSON íŒŒì¼ ê²½ë¡œ
            video_file_path: ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        self.json_file_path = json_file_path
        self.video_file_path = video_file_path
        self.annotation_data = None
        self.video_cap = None
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (track_idë³„ ìƒ‰ìƒ)
        self.color_palette = [
            (255, 0, 0),      # Red
            (0, 255, 0),      # Green  
            (0, 0, 255),      # Blue
            (255, 255, 0),    # Yellow
            (255, 0, 255),    # Magenta
            (0, 255, 255),    # Cyan
            (128, 0, 128),    # Purple
            (255, 165, 0),    # Orange
        ]
        
        self.track_colors = {}
        self.color_index = 0
        
        self.load_annotation_data()
        if video_file_path:
            self.load_video()
    
    def load_annotation_data(self):
        """JSON ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                self.annotation_data = json.load(f)
            print(f"âœ… JSON ë°ì´í„° ë¡œë“œ ì„±ê³µ: {self.json_file_path}")
            self.print_data_summary()
        except Exception as e:
            print(f"âŒ JSON ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        return True
    
    def load_video(self):
        """ë™ì˜ìƒ íŒŒì¼ ë¡œë“œ"""
        try:
            self.video_cap = cv2.VideoCapture(self.video_file_path)
            if not self.video_cap.isOpened():
                print(f"âŒ ë™ì˜ìƒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {self.video_file_path}")
                return False
            print(f"âœ… ë™ì˜ìƒ íŒŒì¼ ë¡œë“œ ì„±ê³µ: {self.video_file_path}")
            return True
        except Exception as e:
            print(f"âŒ ë™ì˜ìƒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def print_data_summary(self):
        """ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        if not self.annotation_data:
            return
        
        video_info = self.annotation_data.get('video_info', {})
        groundings = self.annotation_data.get('groundings', [])
        
        print(f"\nğŸ“Š ì–´ë…¸í…Œì´ì…˜ ë°ì´í„° ìš”ì•½:")
        print(f"   ë¹„ë””ì˜¤: {video_info.get('filename', 'N/A')}")
        print(f"   ì „ì²´ í”„ë ˆì„: {video_info.get('total_frames', 'N/A')}")
        print(f"   FPS: {video_info.get('fps', 'N/A')}")
        print(f"   í•´ìƒë„: {video_info.get('resolution', {}).get('width', 'N/A')} x {video_info.get('resolution', {}).get('height', 'N/A')}")
        print(f"   ê·¸ë¼ìš´ë”© ê°œìˆ˜: {len(groundings)}")
        
        # ê° ê·¸ë¼ìš´ë”©ë³„ ì •ë³´
        for i, grounding in enumerate(groundings):
            time_segment = grounding.get('time_segment', {})
            annotations = grounding.get('annotations', {})
            print(f"\n   ê·¸ë¼ìš´ë”© {i+1}:")
            print(f"     í”„ë ˆì„ ë²”ìœ„: {time_segment.get('start_frame', 'N/A')} ~ {time_segment.get('end_frame', 'N/A')}")
            print(f"     ìƒ˜í”Œë§ëœ í”„ë ˆì„: {time_segment.get('sampled_frames', [])}")
            print(f"     ì–´ë…¸í…Œì´ì…˜ëœ í”„ë ˆì„: {list(annotations.keys())}")
            
            # ê° í”„ë ˆì„ë³„ ê°ì²´ ê°œìˆ˜
            for frame_idx, frame_annotations in annotations.items():
                objects = [ann['object_type'] for ann in frame_annotations]
                track_ids = [ann['track_id'] for ann in frame_annotations]
                print(f"       í”„ë ˆì„ {frame_idx}: {len(frame_annotations)}ê°œ ê°ì²´ ({', '.join(set(objects))}) - Track IDs: {track_ids}")
    
    def get_track_color(self, track_id):
        """Track IDë³„ ìƒ‰ìƒ í• ë‹¹"""
        if track_id not in self.track_colors:
            color = self.color_palette[self.color_index % len(self.color_palette)]
            self.track_colors[track_id] = color
            self.color_index += 1
        return self.track_colors[track_id]
    
    def validate_coordinates(self):
        """ì¢Œí‘œ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        print(f"\nğŸ” ì¢Œí‘œ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦:")
        
        if not self.annotation_data:
            print("âŒ ì–´ë…¸í…Œì´ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        video_info = self.annotation_data.get('video_info', {})
        resolution = video_info.get('resolution', {})
        img_width = resolution.get('width', 0)
        img_height = resolution.get('height', 0)
        
        if img_width == 0 or img_height == 0:
            print("âŒ ë¹„ë””ì˜¤ í•´ìƒë„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"   ë¹„ë””ì˜¤ í•´ìƒë„: {img_width} x {img_height}")
        
        issues = []
        total_bboxes = 0
        
        for grounding in self.annotation_data.get('groundings', []):
            annotations = grounding.get('annotations', {})
            
            for frame_idx, frame_annotations in annotations.items():
                for i, bbox in enumerate(frame_annotations):
                    total_bboxes += 1
                    
                    x = bbox.get('x', 0)
                    y = bbox.get('y', 0)
                    width = bbox.get('width', 0)
                    height = bbox.get('height', 0)
                    track_id = bbox.get('track_id', 'unknown')
                    object_type = bbox.get('object_type', 'unknown')
                    
                    # 1. ìŒìˆ˜ ì¢Œí‘œ ê²€ì‚¬
                    if x < 0 or y < 0:
                        issues.append(f"í”„ë ˆì„ {frame_idx}, {object_type}({track_id}): ìŒìˆ˜ ì¢Œí‘œ (x:{x}, y:{y})")
                    
                    # 2. í¬ê¸° ê²€ì‚¬
                    if width <= 0 or height <= 0:
                        issues.append(f"í”„ë ˆì„ {frame_idx}, {object_type}({track_id}): ì˜ëª»ëœ í¬ê¸° (w:{width}, h:{height})")
                    
                    # 3. ì´ë¯¸ì§€ ê²½ê³„ ê²€ì‚¬
                    if x + width > img_width:
                        issues.append(f"í”„ë ˆì„ {frame_idx}, {object_type}({track_id}): ê°€ë¡œ ê²½ê³„ ì´ˆê³¼ (x+w:{x+width} > {img_width})")
                    
                    if y + height > img_height:
                        issues.append(f"í”„ë ˆì„ {frame_idx}, {object_type}({track_id}): ì„¸ë¡œ ê²½ê³„ ì´ˆê³¼ (y+h:{y+height} > {img_height})")
                    
                    # 4. ë§¤ìš° ì‘ì€ ë°”ìš´ë”© ë°•ìŠ¤ ê²€ì‚¬ (10í”½ì…€ ë¯¸ë§Œ)
                    if width < 10 or height < 10:
                        issues.append(f"í”„ë ˆì„ {frame_idx}, {object_type}({track_id}): ë§¤ìš° ì‘ì€ ë°•ìŠ¤ (w:{width}, h:{height})")
        
        print(f"   ì´ ê²€ì‚¬í•œ ë°”ìš´ë”© ë°•ìŠ¤: {total_bboxes}ê°œ")
        
        if issues:
            print(f"âŒ {len(issues)}ê°œì˜ ë¬¸ì œ ë°œê²¬:")
            for issue in issues[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
                print(f"     - {issue}")
            if len(issues) > 10:
                print(f"     ... ë° {len(issues) - 10}ê°œ ì¶”ê°€ ë¬¸ì œ")
            return False
        else:
            print("âœ… ëª¨ë“  ì¢Œí‘œê°€ ìœ íš¨í•©ë‹ˆë‹¤!")
            return True
    
    def calculate_bbox_statistics(self):
        """ë°”ìš´ë”© ë°•ìŠ¤ í†µê³„ ê³„ì‚°"""
        print(f"\nğŸ“Š Bounding box statistics:")
        
        if not self.annotation_data:
            return
        
        all_widths = []
        all_heights = []
        all_areas = []
        object_counts = {}
        track_counts = {}
        
        for grounding in self.annotation_data.get('groundings', []):
            annotations = grounding.get('annotations', {})
            
            for frame_idx, frame_annotations in annotations.items():
                for bbox in frame_annotations:
                    width = bbox.get('width', 0)
                    height = bbox.get('height', 0)
                    area = width * height
                    object_type = bbox.get('object_type', 'unknown')
                    track_id = bbox.get('track_id', 'unknown')
                    
                    all_widths.append(width)
                    all_heights.append(height)
                    all_areas.append(area)
                    
                    object_counts[object_type] = object_counts.get(object_type, 0) + 1
                    track_counts[track_id] = track_counts.get(track_id, 0) + 1
        
        if all_widths:
            print(f"   Width - Mean: {np.mean(all_widths):.1f}, Min: {np.min(all_widths)}, Max: {np.max(all_widths)}")
            print(f"   Height - Mean: {np.mean(all_heights):.1f}, Min: {np.min(all_heights)}, Max: {np.max(all_heights)}")
            print(f"   Area - Mean: {np.mean(all_areas):.1f}, Min: {np.min(all_areas)}, Max: {np.max(all_areas)}")
            
            print(f"\n   Count by object type:")
            for obj_type, count in sorted(object_counts.items()):
                print(f"     {obj_type}: {count} boxes")
            
            print(f"\n   Count by track ID:")
            for track_id, count in sorted(track_counts.items()):
                print(f"     {track_id}: {count} boxes")
    
    def get_frame_image(self, frame_index):
        """íŠ¹ì • í”„ë ˆì„ì˜ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°"""
        if not self.video_cap:
            return None
        
        self.video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
        ret, frame = self.video_cap.read()
        
        if ret:
            # BGR to RGB ë³€í™˜
            return cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        return None
    
    def visualize_frame_annotations(self, frame_index, save_path=None, show_plot=True):
        """íŠ¹ì • í”„ë ˆì„ì˜ ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™”"""
        print(f"\nğŸ¨ í”„ë ˆì„ {frame_index} ì–´ë…¸í…Œì´ì…˜ ì‹œê°í™”:")
        
        # í•´ë‹¹ í”„ë ˆì„ì˜ ì–´ë…¸í…Œì´ì…˜ ì°¾ê¸°
        frame_annotations = []
        for grounding in self.annotation_data.get('groundings', []):
            annotations = grounding.get('annotations', {})
            if str(frame_index) in annotations:
                frame_annotations.extend(annotations[str(frame_index)])
        
        if not frame_annotations:
            print(f"   í”„ë ˆì„ {frame_index}ì— ì–´ë…¸í…Œì´ì…˜ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        frame_image = None
        if self.video_cap:
            frame_image = self.get_frame_image(frame_index)
        
        # ë¹„ë””ì˜¤ í•´ìƒë„ ì •ë³´
        video_info = self.annotation_data.get('video_info', {})
        resolution = video_info.get('resolution', {})
        img_width = resolution.get('width', 1280)
        img_height = resolution.get('height', 720)
        
        # ì‹œê°í™”
        fig, ax = plt.subplots(1, 1, figsize=(12, 8))
        
        if frame_image is not None:
            ax.imshow(frame_image)
            print(f"   ì‹¤ì œ ë¹„ë””ì˜¤ í”„ë ˆì„ í‘œì‹œ (í•´ìƒë„: {frame_image.shape[1]}x{frame_image.shape[0]})")
        else:
            # ë¹„ë””ì˜¤ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ìº”ë²„ìŠ¤
            ax.set_xlim(0, img_width)
            ax.set_ylim(img_height, 0)  # Yì¶• ë’¤ì§‘ê¸° (ì´ë¯¸ì§€ ì¢Œí‘œê³„)
            ax.set_aspect('equal')
            print(f"   ë¹„ë””ì˜¤ ì—†ìŒ - ì¢Œí‘œë§Œ í‘œì‹œ (í•´ìƒë„: {img_width}x{img_height})")
        
        # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        for i, bbox in enumerate(frame_annotations):
            x = bbox.get('x', 0)
            y = bbox.get('y', 0)
            width = bbox.get('width', 0)
            height = bbox.get('height', 0)
            track_id = bbox.get('track_id', 'unknown')
            object_type = bbox.get('object_type', 'unknown')
            
            # ìƒ‰ìƒ ê°€ì ¸ì˜¤ê¸°
            color = self.get_track_color(track_id)
            color_normalized = tuple(c/255.0 for c in color)
            
            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            rect = patches.Rectangle(
                (x, y), width, height,
                linewidth=2, edgecolor=color_normalized, facecolor='none'
            )
            ax.add_patch(rect)
            
            # ë¼ë²¨ ì¶”ê°€
            ax.text(x, y-5, f"{object_type}-{track_id}", 
                   color=color_normalized, fontsize=10, fontweight='bold',
                   bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8))
            
            print(f"     {object_type}({track_id}): x={x}, y={y}, w={width}, h={height}")
        
        ax.set_title(f"Frame {frame_index} - {len(frame_annotations)} bounding boxes")
        ax.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            print(f"   ì‹œê°í™” ì €ì¥: {save_path}")
        
        if show_plot:
            plt.show()
        else:
            plt.close()
    
    def visualize_all_annotated_frames(self, output_dir="validation_output", show_plots=False):
        """ëª¨ë“  ì–´ë…¸í…Œì´ì…˜ëœ í”„ë ˆì„ ì‹œê°í™”"""
        print(f"\nğŸ¨ ëª¨ë“  ì–´ë…¸í…Œì´ì…˜ëœ í”„ë ˆì„ ì‹œê°í™”:")
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        annotated_frames = set()
        for grounding in self.annotation_data.get('groundings', []):
            annotations = grounding.get('annotations', {})
            annotated_frames.update(int(frame_idx) for frame_idx in annotations.keys())
        
        print(f"   ì´ {len(annotated_frames)}ê°œ í”„ë ˆì„ ì²˜ë¦¬ ì¤‘...")
        
        for frame_idx in sorted(annotated_frames):
            save_path = output_path / f"frame_{frame_idx:03d}.png"
            self.visualize_frame_annotations(frame_idx, save_path=save_path, show_plot=show_plots)
        
        print(f"âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ - ì €ì¥ ê²½ë¡œ: {output_path}")
    
    def validate_coordinate_transformations(self):
        """ì¢Œí‘œ ë³€í™˜ ë¡œì§ ê²€ì¦ (VideoCanvas ì½”ë“œ ê¸°ë°˜)"""
        print(f"\nğŸ”§ Coordinate transformation validation:")
        
        video_info = self.annotation_data.get('video_info', {})
        resolution = video_info.get('resolution', {})
        original_width = resolution.get('width', 1280)
        original_height = resolution.get('height', 720)
        
        print(f"   Original video resolution: {original_width} x {original_height}")
        
        # Test with various canvas sizes
        test_canvas_sizes = [
            (640, 360),   # Half size
            (1920, 1080), # Larger size
            (800, 600),   # Different aspect ratio
            (500, 300),   # Small size
        ]
        
        print(f"\n   Coordinate transformation test with various canvas sizes:")
        
        for canvas_width, canvas_height in test_canvas_sizes:
            print(f"\n     Canvas size: {canvas_width} x {canvas_height}")
            
            # VideoCanvas scale_factor calculation logic
            scale_factor = min(
                canvas_width / original_width,
                canvas_height / original_height
            )
            
            # Scaled image size
            scaled_width = int(original_width * scale_factor)
            scaled_height = int(original_height * scale_factor)
            
            # Offset calculation (image centered in canvas)
            x_offset = (canvas_width - scaled_width) // 2
            y_offset = (canvas_height - scaled_height) // 2
            
            print(f"       Scale factor: {scale_factor:.4f}")
            print(f"       Scaled image: {scaled_width} x {scaled_height}")
            print(f"       Offset: ({x_offset}, {y_offset})")
            
            # Test coordinates
            test_coords = [
                (0, 0),                                    # Top-left
                (original_width, original_height),         # Bottom-right
                (original_width//2, original_height//2),   # Center
            ]
            
            print(f"       Coordinate transformation test:")
            for img_x, img_y in test_coords:
                # Image to Canvas transformation
                canvas_x = int(img_x * scale_factor + x_offset)
                canvas_y = int(img_y * scale_factor + y_offset)
                
                # Canvas to Image reverse transformation
                img_x_back = int((canvas_x - x_offset) / scale_factor)
                img_y_back = int((canvas_y - y_offset) / scale_factor)
                
                error_x = abs(img_x - img_x_back)
                error_y = abs(img_y - img_y_back)
                
                print(f"         Image({img_x},{img_y}) -> Canvas({canvas_x},{canvas_y}) -> Image({img_x_back},{img_y_back}) | Error: ({error_x},{error_y})")
                
                if error_x > 1 or error_y > 1:
                    print(f"         âš ï¸  Large coordinate transformation error!")
        
        print(f"\nâœ… Coordinate transformation logic validation completed")
    
    def run_full_validation(self, output_dir="validation_output"):
        """ì „ì²´ ê²€ì¦ ì‹¤í–‰"""
        print("ğŸš€ ë°”ìš´ë”© ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì „ì²´ ê²€ì¦ ì‹œì‘\n")
        
        # 1. ì¢Œí‘œ ìœ íš¨ì„± ê²€ì¦
        coord_valid = self.validate_coordinates()
        
        # 2. í†µê³„ ê³„ì‚°
        self.calculate_bbox_statistics()
        
        # 3. ì¢Œí‘œ ë³€í™˜ ë¡œì§ ê²€ì¦
        self.validate_coordinate_transformations()
        
        # 4. ì‹œê°í™”
        self.visualize_all_annotated_frames(output_dir=output_dir, show_plots=False)
        
        # 5. ìƒ˜í”Œ í”„ë ˆì„ ìƒì„¸ í‘œì‹œ
        annotated_frames = set()
        for grounding in self.annotation_data.get('groundings', []):
            annotations = grounding.get('annotations', {})
            annotated_frames.update(int(frame_idx) for frame_idx in annotations.keys())
        
        if annotated_frames:
            sample_frame = min(annotated_frames)
            print(f"\nğŸ‘€ ìƒ˜í”Œ í”„ë ˆì„ {sample_frame} ìƒì„¸ ì‹œê°í™”:")
            self.visualize_frame_annotations(sample_frame, show_plot=True)
        
        # 6. ìµœì¢… ê²°ê³¼
        print(f"\nğŸ¯ Validation results summary:")
        print(f"   Coordinate validity: {'âœ… PASSED' if coord_valid else 'âŒ FAILED'}")
        print(f"   Visualization files: Saved in {output_dir}/ directory")
        print(f"   Coordinate transformation: âœ… Validated")
        
        if coord_valid:
            print(f"\nğŸ‰ All validations completed successfully!")
            print(f"   Your annotation framework's bounding box storage is accurate.")
        else:
            print(f"\nâš ï¸  Some issues were found. Please check the error messages above.")

def main():
    parser = argparse.ArgumentParser(description='ë°”ìš´ë”© ë°•ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ê²€ì¦ ë„êµ¬')
    parser.add_argument('json_file', help='ì–´ë…¸í…Œì´ì…˜ JSON íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--video', help='ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--output', default='validation_output', help='ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: validation_output)')
    
    args = parser.parse_args()
    
    # ê²€ì¦ ì‹¤í–‰
    validator = BBoxValidator(args.json_file, args.video)
    validator.run_full_validation(args.output)

if __name__ == "__main__":
    main()